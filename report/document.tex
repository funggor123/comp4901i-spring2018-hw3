\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}


% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{COMP4901I - Opinion Mining Report}

\author{%
	Cheng Chi Fung \\
	\texttt{cfchengac@connect.ust.hk} \\
}

\begin{document}

\maketitle

\section{Data}

\subsection{Data Cleaning}
In this assignments, the following data cleaning methods is used. First, turn all the string into lower case and turn all of them into ascii encoding. It is followed by expanding the contradiction. Morverover, we remove all the digits and special characters.

\subsection{Data Statistics}
The following are the data statistics of the dataset given.

\begin{table}[htb]
	\caption{Data Statistics}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Statistics & --  \\
		\midrule
		Number of sentence & 1 \\
		Number of words & 2  \\
		Number of vocabs & 3 \\
		Frequent words & 4  \\
		Max sentence length & 5  \\
		Average sentence length & 6  \\
		Std sentence length & 7  \\
		\bottomrule
	\end{tabular}
\end{table}

\section{Implement ConvNet with PyTorch}
The following are the data statistics of the dataset given.

\begin{table}[htb]
	\caption{Development accuracy and Test Set accuracy}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset &Accuracy\\
		\midrule
		Test Set & 0.01   \\
		Development Set & 0.01  \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Hyperparameters Tuning Results}
The following are the hyperparameters tuning results.

\begin{table}[htb]
	\caption{Char CNN Archiecture we used}
	\label{sample-table}
	\centering
	\begin{tabular}{lllllll}
		\toprule
		\cmidrule{1-7}
		Pooling Types & Learning Rate & Kernel Size & Dropout rate & Embedding Dimension 	& Number of Filters & Results\\
		\midrule
		Average Pooling & 0.01  & 100 & 0.1 & 100 & 2 & 32 \\
		\bottomrule
	\end{tabular}
\end{table}

\pagebreak

\section{Results and Analysis}

\subsection{Development Set Accuracy and Test set Accuracy}

\begin{table}[htb]
	\caption{Development accuracy and Test Set accuracy}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset &Accuracy\\
		\midrule
		Test Set & 0.01   \\
		Development Set & 0.01  \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Analysis}
For the kernel sizes, we found that for larger kernel size, we need to train more iterations but getting converge.

For the dropout, we found out that, the it requires more time to converge but getting higher best test set accuracy. The reason for that may be due to dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.

We found out that for higher learning rate, the higher the convergnce rate. However, 
\section{Bonus}

\subsection{Dynamic Padding}
The following are the screen shot of the Predictor Network, CNN Encoder Network and loading the pretrained encoder.

\subsection{Pretrained Word Embedding}
For Pretrained Word Embedding, we have tried to replace the original word embedding layer by the pretrained \textbf{word2Vector} with \textbf{Google News corpus} (3 billion running words) word vector model. (Google News Corpus: https://github.com/mmihaltz/word2vec-GoogleNews-vectors). And since the dimension of the embedding matrix is enormously big which cause some memory error while training, we have limited to only use ten thousands of vocabs. All the above process can be easily done through by a python libarary named \textbf{gensim}.

And the following are the results of using pretrained embedding.

\begin{table}[htb]
	\caption{Development accuracy and Test Set accuracy}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset &Accuracy\\
		\midrule
		Test Set & 0.01   \\
		Development Set & 0.01  \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Other CNN Architectures}
For other CNN archiectures, we have implmented character CNN by following the paper \textbf{Character-level Convolutional Networks for Text Classification}. (https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf) 

Same as the paper, we have defined a list of characters which includes 26 English letters, 10 digits, 34 special characters and one blank characters. (\textbf{70 Characters in total})

In the later part, we transfer those characters as 1-hot encoding and used it to create the sentence vectors for each sentences. For unknown characters, blank characters are used to replace it. The sentence vectors would then be inputed into the CNN with the following archiecture which is quite similiar to the paper.

\pagebreak

\begin{table}[htb]
\caption{Char CNN Archiecture we used}
	\label{sample-table}
	\centering
\begin{tabular}{lllll}
\toprule
		\cmidrule{1-5}
		Layer & Layer types & Kernel Size & Pooling Size / is Dropout & Number of Filters 		\\
		\midrule
 			1 & Embedding & 100 & -- & -- \\
 			2 & Conv2d & 7 & 3 & 256 \\
 			3 & Conv1d & 7 & 3 & 256 \\
 			4 & Conv1d & 3 & -- & 256 \\
 			5 & Conv1d & 3 & -- & 256\\
 			6 & Conv1d & 3 & -- & 256 \\
 			7 & Conv1d & 3 & 3 & 256 \\
 			8 & Linear & 1024 & Yes & -- \\
 			9 & Linear & 1024 & Yes & -- \\
 			10 & Linear & 3 & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

And the following are the results of using Char CNN. 

\begin{table}[htb]
	\caption{Development accuracy and Test Set accuracy}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset &Accuracy\\
		\midrule
		Test Set & 0.01   \\
		Development Set & 0.01  \\
		\bottomrule
	\end{tabular}
\end{table}

\end{document}
